"""
é…ç½®é¡µé¢
"""

import streamlit as st
from utils.session import save_config


def show():
    """æ˜¾ç¤ºé…ç½®é¡µé¢"""
    
    st.title("âš™ï¸ ç³»ç»Ÿé…ç½®")
    
    st.markdown("""
    é…ç½® OpenAI å…¼å®¹çš„ API ä»¥å¼€å§‹ä½¿ç”¨ã€‚æ”¯æŒ OpenAIã€Azure OpenAIã€æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰ç­‰ã€‚
    """)
    
    st.markdown("---")
    
    # API Key é…ç½®
    st.subheader("ğŸ”‘ API é…ç½®")
    
    api_key = st.text_input(
        "API Key",
        value=st.session_state.get('api_key', ''),
        type="password",
        help="OpenAI API Key æˆ–å…¼å®¹æ ¼å¼çš„ API Key"
    )
    
    api_base_url = st.text_input(
        "API Base URL",
        value=st.session_state.get('api_base_url', 'https://api.openai.com/v1'),
        help="API ç«¯ç‚¹åœ°å€ï¼Œå¦‚ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¯ä¿®æ”¹ä¸º http://localhost:11434/v1"
    )
    
    model = st.text_input(
        "æ¨¡å‹åç§°",
        value=st.session_state.get('model', 'gpt-3.5-turbo'),
        help="æ¨¡å‹åç§°ï¼Œå¦‚ gpt-4, gpt-3.5-turbo, llama2 ç­‰"
    )
    
    col1, col2 = st.columns([1, 4])
    
    with col1:
        if st.button("ğŸ’¾ ä¿å­˜é…ç½®", type="primary"):
            st.session_state.api_key = api_key
            st.session_state.api_base_url = api_base_url
            st.session_state.model = model
            save_config()
            st.success("âœ… é…ç½®å·²ä¿å­˜ï¼")
            st.rerun()
    
    with col2:
        if st.button("ğŸ§ª æµ‹è¯•è¿æ¥"):
            if not api_key:
                st.error("âŒ è¯·å…ˆè¾“å…¥ API Key")
            else:
                with st.spinner("æµ‹è¯•è¿æ¥ä¸­..."):
                    # æµ‹è¯• API è¿æ¥
                    try:
                        import sys
                        import os
                        sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
                        from ame.llm_caller.caller import LLMCaller
                        
                        llm = LLMCaller(api_key=api_key, base_url=api_base_url, model=model)
                        # ç®€å•æµ‹è¯•
                        response = llm.generate(
                            messages=[{"role": "user", "content": "Hi"}],
                            temperature=0.7,
                            max_tokens=10
                        )
                        st.success("âœ… è¿æ¥æˆåŠŸï¼")
                    except Exception as e:
                        st.error(f"âŒ è¿æ¥å¤±è´¥: {str(e)}")
    
    st.markdown("---")
    
    # æ•°æ®å­˜å‚¨é…ç½®
    st.subheader("ğŸ’¾ æ•°æ®å­˜å‚¨")
    
    st.info("""
    - ğŸ“ æ•°æ®ç›®å½•: `./data/`
    - ğŸ“¦ å‘é‡åº“: `./data/vector_store/`
    - ğŸ“ ä¸Šä¼ æ–‡ä»¶: `./data/uploads/`
    - âš™ï¸ é…ç½®æ–‡ä»¶: `./data/runtime_config.json`
    """)
    
    st.markdown("---")
    
    # ç³»ç»Ÿä¿¡æ¯
    st.subheader("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ç‰ˆæœ¬", "v0.4.0")
    
    with col2:
        st.metric("AME å¼•æ“", "v0.4.0")
    
    with col3:
        status = "âœ… å·²é…ç½®" if st.session_state.is_configured else "âš ï¸ æœªé…ç½®"
        st.metric("çŠ¶æ€", status)
