"""
MEM å¯¹è¯é¡µé¢ç»„ä»¶ - æ”¯æŒæµå¼è¾“å‡º
"""

import gradio as gr
import sys
import os
import asyncio

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from ame.mem.mimic_engine import MimicEngine
from ame.llm_caller.caller import LLMCaller
from utils.session import get_session_state, update_session_state


def init_mimic_engine():
    """åˆå§‹åŒ–æ¨¡ä»¿å¼•æ“"""
    state = get_session_state()
    
    if state.get('mimic_engine') is None:
        llm_caller = LLMCaller(
            api_key=state.get('api_key'),
            base_url=state.get('api_base_url'),
            model=state.get('model')
        )
        engine = MimicEngine(llm_caller=llm_caller)
        update_session_state('mimic_engine', engine)
        return engine
    
    return state.get('mimic_engine')


def get_statistics():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    try:
        engine = init_mimic_engine()
        stats = asyncio.run(engine.vector_store.get_statistics())
        return (
            stats.get('count', 0),
            len(stats.get('sources', {})),
            "âœ… æ­£å¸¸"
        )
    except:
        return 0, 0, "âš ï¸ æœªåˆå§‹åŒ–"


def chat_response(message, history, temperature):
    """ç”Ÿæˆå¯¹è¯å“åº”ï¼ˆæµå¼ï¼‰"""
    state = get_session_state()
    
    if not state.get('is_configured'):
        yield "âš ï¸ è¯·å…ˆåœ¨é…ç½®é¡µé¢è®¾ç½® API Key"
        return
    
    if not message.strip():
        yield "è¯·è¾“å…¥æ¶ˆæ¯..."
        return
    
    try:
        engine = init_mimic_engine()
        
        # æµå¼ç”Ÿæˆ
        full_response = ""
        
        async def generate():
            nonlocal full_response
            async for chunk in engine.generate_response_stream(
                prompt=message,
                temperature=temperature,
                use_history=True
            ):
                full_response += chunk
                yield full_response
        
        # è¿è¡Œå¼‚æ­¥ç”Ÿæˆå™¨
        for response in asyncio.run(generate()):
            yield response
        
        # å­¦ä¹ è¿™æ¬¡å¯¹è¯
        asyncio.run(engine.learn_from_conversation(
            user_message=message,
            context=full_response
        ))
        
    except Exception as e:
        yield f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def upload_and_learn(file, progress=gr.Progress()):
    """ä¸Šä¼ å¹¶å­¦ä¹ èŠå¤©è®°å½•"""
    state = get_session_state()
    
    if not state.get('is_configured'):
        return "âš ï¸ è¯·å…ˆåœ¨é…ç½®é¡µé¢è®¾ç½® API Key"
    
    if file is None:
        return "è¯·å…ˆä¸Šä¼ æ–‡ä»¶"
    
    try:
        engine = init_mimic_engine()
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file.name, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŒ‰è¡Œåˆ†å‰²
        lines = content.split('\n')
        messages = [line.strip() for line in lines if line.strip()]
        
        # å­¦ä¹ æ¯æ¡æ¶ˆæ¯
        for i, msg in enumerate(messages):
            progress((i + 1) / len(messages), desc=f"å­¦ä¹ ä¸­ {i+1}/{len(messages)}")
            asyncio.run(engine.learn_from_conversation(
                user_message=msg,
                metadata={"source": "uploaded_chat"}
            ))
        
        return f"âœ… å·²å­¦ä¹  {len(messages)} æ¡æ¶ˆæ¯ï¼"
    
    except Exception as e:
        return f"âŒ å­¦ä¹ å¤±è´¥: {str(e)}"


def create_mem_tab():
    """åˆ›å»º MEM å¯¹è¯é¡µé¢"""
    
    gr.Markdown(
        """
        ## ğŸ’¬ MEM å¯¹è¯
        
        ä¸ AI åˆ†èº«å¯¹è¯ï¼Œå®ƒä¼šæ¨¡ä»¿ä½ çš„è¯´è¯é£æ ¼ã€‚ä¸Šä¼ èŠå¤©è®°å½•è®©å®ƒå­¦ä¹ ï¼Œå¯¹è¯ä¼šè¢«è®°å½•ä»¥æŒç»­æ”¹è¿›ã€‚
        """
    )
    
    # ç»Ÿè®¡ä¿¡æ¯å¡ç‰‡
    with gr.Row():
        memory_count = gr.Number(label="ğŸ’¬ æ€»è®°å¿†æ•°", value=0, interactive=False)
        source_count = gr.Number(label="ğŸ·ï¸ æ¥æºç±»å‹", value=0, interactive=False)
        status_text = gr.Textbox(label="ğŸ“Š çŠ¶æ€", value="", interactive=False)
        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡", size="sm")
    
    refresh_btn.click(
        fn=get_statistics,
        outputs=[memory_count, source_count, status_text]
    )
    
    gr.Markdown("---")
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    with gr.Tabs():
        
        # å¯¹è¯æ ‡ç­¾é¡µ
        with gr.TabItem("ğŸ’¬ å¯¹è¯"):
            
            # èŠå¤©ç•Œé¢
            chatbot = gr.Chatbot(
                label="AI åˆ†èº«å¯¹è¯",
                height=500,
                avatar_images=(None, "ğŸ¤–"),
                bubble_full_width=False
            )
            
            with gr.Row():
                with gr.Column(scale=4):
                    msg_input = gr.Textbox(
                        placeholder="è¾“å…¥æ¶ˆæ¯...",
                        show_label=False,
                        container=False
                    )
                with gr.Column(scale=1):
                    temperature_slider = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.8,
                        step=0.1,
                        label="æ¸©åº¦",
                        info="æ§åˆ¶å›å¤çš„éšæœºæ€§"
                    )
            
            with gr.Row():
                send_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary")
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")
            
            # ç»‘å®šäº‹ä»¶
            msg_input.submit(
                fn=chat_response,
                inputs=[msg_input, chatbot, temperature_slider],
                outputs=[chatbot],
            )
            
            send_btn.click(
                fn=chat_response,
                inputs=[msg_input, chatbot, temperature_slider],
                outputs=[chatbot],
            )
            
            clear_btn.click(
                fn=lambda: [],
                outputs=[chatbot]
            )
        
        # å­¦ä¹ ææ–™æ ‡ç­¾é¡µ
        with gr.TabItem("ğŸ“š å­¦ä¹ ææ–™"):
            
            gr.Markdown(
                """
                ### ä¸Šä¼ èŠå¤©è®°å½•
                
                ä¸Šä¼ ä½ çš„èŠå¤©è®°å½•æ–‡æœ¬æ–‡ä»¶ï¼Œè®© AI å­¦ä¹ ä½ çš„è¯´è¯é£æ ¼ã€‚
                
                **æ”¯æŒæ ¼å¼**ï¼š
                - çº¯æ–‡æœ¬ï¼ˆæ¯è¡Œä¸€æ¡æ¶ˆæ¯ï¼‰
                - JSON æ ¼å¼çš„èŠå¤©è®°å½•
                """
            )
            
            file_upload = gr.File(
                label="é€‰æ‹©èŠå¤©è®°å½•æ–‡ä»¶",
                file_types=[".txt", ".json"],
                type="filepath"
            )
            
            upload_btn = gr.Button("ğŸ“š å¼€å§‹å­¦ä¹ ", variant="primary")
            upload_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
            
            upload_btn.click(
                fn=upload_and_learn,
                inputs=[file_upload],
                outputs=[upload_status]
            )
            
            gr.Markdown("---")
            
            gr.Markdown("### âœï¸ æ‰‹åŠ¨è¾“å…¥")
            
            manual_text = gr.Textbox(
                label="è¾“å…¥ä¸€æ®µä½ è¯´è¿‡çš„è¯",
                placeholder="è¾“å…¥ä½ çš„èŠå¤©è®°å½•ã€æ—¥è®°ã€æƒ³æ³•...",
                lines=5
            )
            
            manual_btn = gr.Button("ğŸ’¾ ä¿å­˜å¹¶å­¦ä¹ ")
            manual_status = gr.Textbox(label="çŠ¶æ€", interactive=False, show_label=False)
            
            def learn_manual(text):
                if not text.strip():
                    return "è¯·è¾“å…¥å†…å®¹"
                
                try:
                    engine = init_mimic_engine()
                    asyncio.run(engine.learn_from_conversation(
                        user_message=text,
                        metadata={"source": "manual_input"}
                    ))
                    return "âœ… å·²å­¦ä¹ ï¼"
                except Exception as e:
                    return f"âŒ å¤±è´¥: {str(e)}"
            
            manual_btn.click(
                fn=learn_manual,
                inputs=[manual_text],
                outputs=[manual_status]
            )
    
    # åŠ è½½æ—¶åˆ·æ–°ç»Ÿè®¡
    gr.on(
        triggers=[],
        fn=get_statistics,
        outputs=[memory_count, source_count, status_text]
    )
