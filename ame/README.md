# AME - Another Me Engine

**AME (Another Me Engine)** æ˜¯ Another Me é¡¹ç›®çš„ç‹¬ç«‹æŠ€æœ¯æ¨¡å—å¼•æ“ï¼Œæä¾›äº†æ‰€æœ‰æ ¸å¿ƒæŠ€æœ¯åŠŸèƒ½çš„å®ç°ã€‚

**ç‰ˆæœ¬**: v1.0.0

## ğŸ¯ è®¾è®¡ç†å¿µ

AME é‡‡ç”¨å®Œå…¨æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„è®¾è®¡ï¼Œæ¯ä¸ªæ¨¡å—éƒ½æ˜¯ç‹¬ç«‹çš„æŠ€æœ¯å•å…ƒï¼Œå…·æœ‰æ˜ç¡®çš„è¾“å…¥è¾“å‡ºæ¥å£ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- âœ… **æŠ½è±¡åŸºç±»**ï¼šæ‰€æœ‰æ¨¡å—éƒ½æä¾›æŠ½è±¡åŸºç±»ï¼Œæ”¯æŒè‡ªå®šä¹‰å®ç°
- âœ… **å·¥å‚æ¨¡å¼**ï¼šé€šè¿‡å·¥å‚ç±»åˆ›å»ºå®ä¾‹ï¼Œæ”¯æŒåŠ¨æ€åˆ‡æ¢
- âœ… **ç‹¬ç«‹å¯æµ‹**ï¼šæ¨¡å—ä¹‹é—´ä½è€¦åˆï¼Œä¾¿äºå•å…ƒæµ‹è¯•
- âœ… **æ˜“äºå¤ç”¨**ï¼šå¯åœ¨å…¶ä»–é¡¹ç›®ä¸­ç›´æ¥å¼•ç”¨
- âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šå†…ç½®ç¼“å­˜ã€é‡è¯•ã€å¹¶å‘å¤„ç†

## ğŸ“¦ æ ¸å¿ƒæ¨¡å—

### 1. Data Processor (æ•°æ®å¤„ç†æ¨¡å—)
- **processor.py**: åŸºç¡€æ•°æ®å¤„ç†å™¨ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰å¤šç§æ ¼å¼
- **analyzer.py**: æ•°æ®åˆ†æå™¨ï¼Œæä¾›æƒ…ç»ªåˆ†æã€å…³é”®è¯æå–ã€å…³ç³»åˆ†æç­‰åŠŸèƒ½
- **async_processor.py**: å¼‚æ­¥æ•°æ®å¤„ç†å™¨ï¼Œæ”¯æŒå¹¶å‘æ‰¹é‡å¤„ç†

### 2. Vector Store (å‘é‡å­˜å‚¨æ¨¡å—)
- **base.py**: å‘é‡å­˜å‚¨æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£
- **memu_store.py**: Memu å‘é‡å­˜å‚¨å®ç°ï¼ˆè½»é‡çº§ï¼‰
- **store.py**: ChromaDB å‘é‡å­˜å‚¨å®ç°ï¼ˆåŠŸèƒ½å®Œæ•´ï¼‰
- **factory.py**: å·¥å‚æ¨¡å¼ï¼Œæ”¯æŒåŠ¨æ€åˆ‡æ¢å‘é‡å­˜å‚¨å¼•æ“

### 3. LLM Caller (LLMè°ƒç”¨æ¨¡å—)
- **caller.py**: LLMè°ƒç”¨å°è£…ï¼Œæ”¯æŒOpenAIæ ¼å¼API
- ç‰¹æ€§ï¼šé‡è¯•æœºåˆ¶ã€ç¼“å­˜æ”¯æŒã€æµå¼è¾“å‡ºã€é”™è¯¯å¤„ç†

### 4. RAG Generator (RAGç”Ÿæˆæ¨¡å—)
- **generator.py**: æ£€ç´¢å¢å¼ºç”Ÿæˆå™¨ï¼Œç»“åˆå‘é‡æ£€ç´¢å’ŒLLMç”Ÿæˆ

### 5. Retrieval (æ£€ç´¢æ¨¡å—) âœ¨ **v0.3.0 æ–°å¢**
- **base.py**: æ£€ç´¢å™¨æŠ½è±¡åŸºç±» `RetrieverBase`
- **vector_retriever.py**: çº¯å‘é‡æ£€ç´¢
- **hybrid_retriever.py**: **æ··åˆæ£€ç´¢**ï¼ˆå‘é‡ + å…³é”®è¯ + æ—¶é—´åŠ æƒï¼‰
- **reranker.py**: **é‡æ’åºå™¨**ï¼ˆå¤šæ ·æ€§/æ—¶æ•ˆæ€§/LLMé‡æ’åºï¼‰
- **factory.py**: å·¥å‚æ¨¡å¼ï¼Œåˆ›å»ºæ£€ç´¢å™¨å’Œé‡æ’åºå™¨

**å¤æ‚å¬å›ç¤ºä¾‹**ï¼š
```python
from ame import RetrieverFactory

# æ··åˆæ£€ç´¢ï¼šå‘é‡+å…³é”®è¯+æ—¶é—´
retriever = RetrieverFactory.create_retriever(
    retriever_type="hybrid",
    vector_store=vector_store,
    vector_weight=0.6,
    keyword_weight=0.3,
    time_weight=0.1
)
results = await retriever.retrieve(query="...", top_k=10)

# é‡æ’åº
reranker = RetrieverFactory.create_reranker("diversity")
reranked = await reranker.rerank(query="...", results=results)
```

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€ä½¿ç”¨

```python
from ame import DataProcessor, VectorStoreFactory, LLMCaller, RAGGenerator

# 1. æ•°æ®å¤„ç†
processor = DataProcessor()
processed_data = processor.process_file("/path/to/file.txt")

# 2. å‘é‡å­˜å‚¨
vector_store = VectorStoreFactory.create(store_type="memu")
vector_store.add_documents(processed_data)

# 3. LLMè°ƒç”¨
llm = LLMCaller(api_key="your-api-key")
response = llm.generate("ä½ å¥½ï¼Œä¸–ç•Œ")

# 4. RAGç”Ÿæˆ
rag = RAGGenerator(vector_store=vector_store, llm_caller=llm)
answer = rag.generate_answer(query="æˆ‘çš„å…´è¶£çˆ±å¥½æ˜¯ä»€ä¹ˆï¼Ÿ")
```

### 2. RAG çŸ¥è¯†åº“ç®¡ç†

```python
from ame.rag import KnowledgeBase
from ame.vector_store import VectorStoreFactory

# åˆ›å»ºçŸ¥è¯†åº“
vector_store = VectorStoreFactory.create("memu")
kb = KnowledgeBase(
    vector_store=vector_store,
    collection_name="my_knowledge"
)

# æ·»åŠ çŸ¥è¯†
kb.add_documents(
    documents=[
        {"content": "æ–‡æ¡£å†…å®¹1", "metadata": {"source": "file1.txt"}},
        {"content": "æ–‡æ¡£å†…å®¹2", "metadata": {"source": "file2.txt"}}
    ]
)

# æœç´¢çŸ¥è¯†
results = kb.search(query="æœç´¢å…³é”®è¯", top_k=5)

# è·å–æ‰€æœ‰æ–‡æ¡£
all_docs = kb.get_all_documents()

# åˆ é™¤æ–‡æ¡£
kb.delete_documents(ids=["doc_id_1", "doc_id_2"])
```

### 3. MEM è®°å¿†æ¨¡ä»¿

```python
from ame.mem import MimicEngine
from ame.vector_store import VectorStoreFactory
from ame.llm_caller import LLMCaller

# åˆ›å»ºæ¨¡ä»¿å¼•æ“
vector_store = VectorStoreFactory.create("memu")
llm = LLMCaller(api_key="your-api-key", model="gpt-3.5-turbo")

engine = MimicEngine(
    vector_store=vector_store,
    llm_caller=llm,
    collection_name="my_memory"
)

# å­¦ä¹ è®°å¿†
engine.learn_from_texts(
    texts=[
        "ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œå¿ƒæƒ…ä¹Ÿå¾ˆå¥½ã€‚",
        "å’Œæœ‹å‹å»å’–å•¡åº—èŠäº†å¾ˆä¹…ã€‚"
    ],
    metadata={"source": "diary", "date": "2024-10-25"}
)

# ç”Ÿæˆå›å¤ï¼ˆåŒæ­¥ï¼‰
response = engine.generate_response(
    prompt="ä»Šå¤©æ€ä¹ˆæ ·ï¼Ÿ",
    temperature=0.7
)

# ç”Ÿæˆå›å¤ï¼ˆæµå¼ï¼‰
async for chunk in engine.generate_response_stream(
    prompt="ä»Šå¤©æ€ä¹ˆæ ·ï¼Ÿ",
    temperature=0.7
):
    print(chunk, end="", flush=True)

# è·å–è®°å¿†
memories = engine.get_memories(limit=10)
```

### 4. å¤æ‚æ£€ç´¢ç­–ç•¥

```python
from ame.retrieval import RetrieverFactory, RerankerFactory
from ame.vector_store import VectorStoreFactory

# åˆ›å»ºå‘é‡å­˜å‚¨
vector_store = VectorStoreFactory.create("memu")

# æ··åˆæ£€ç´¢ï¼šå‘é‡ + å…³é”®è¯ + æ—¶é—´
retriever = RetrieverFactory.create_retriever(
    retriever_type="hybrid",
    vector_store=vector_store,
    vector_weight=0.6,    # å‘é‡æ£€ç´¢æƒé‡
    keyword_weight=0.3,   # å…³é”®è¯æƒé‡
    time_weight=0.1       # æ—¶é—´æƒé‡
)

results = await retriever.retrieve(query="æœç´¢å…³é”®è¯", top_k=10)

# é‡æ’åºï¼šæå‡å¤šæ ·æ€§
reranker = RerankerFactory.create_reranker("diversity")
reranked = await reranker.rerank(query="æœç´¢å…³é”®è¯", results=results, top_k=5)

# æ—¶æ•ˆæ€§é‡æ’åº
time_reranker = RerankerFactory.create_reranker("recency")
recent_results = await time_reranker.rerank(query="æœç´¢å…³é”®è¯", results=results)
```

### 5. å¼‚æ­¥æ•°æ®å¤„ç†

```python
from ame.data_processor import AsyncDataProcessor

# åˆ›å»ºå¼‚æ­¥å¤„ç†å™¨
processor = AsyncDataProcessor(max_workers=4)

# æ‰¹é‡å¤„ç†æ–‡ä»¶
files = ["/path/to/file1.txt", "/path/to/file2.txt", "/path/to/file3.txt"]
results = await processor.process_files_concurrent(files)

# æ–‡æœ¬åˆ†æ
from ame.data_processor import DataAnalyzer

analyzer = DataAnalyzer()
analysis = analyzer.analyze_text(
    text="ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œå¿ƒæƒ…ä¹Ÿå¾ˆå¥½ã€‚",
    analysis_types=["emotion", "keywords"]
)

print(analysis["emotion"])    # æƒ…ç»ªåˆ†æç»“æœ
print(analysis["keywords"])  # å…³é”®è¯æå–
```

### 6. LLM è°ƒç”¨é«˜çº§ç‰¹æ€§

```python
from ame.llm_caller import LLMCaller

# åˆ›å»º LLM è°ƒç”¨å™¨ï¼ˆå¸¦é‡è¯•å’Œç¼“å­˜ï¼‰
llm = LLMCaller(
    api_key="your-api-key",
    base_url="https://api.openai.com/v1",
    model="gpt-3.5-turbo",
    max_retries=3,        # æœ€å¤§é‡è¯•æ¬¡æ•°
    enable_cache=True     # å¯ç”¨ç¼“å­˜
)

# åŒæ­¥è°ƒç”¨
response = llm.generate(
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼"}
    ],
    temperature=0.7
)

# å¼‚æ­¥æµå¼è°ƒç”¨
async for chunk in llm.generate_stream(
    messages=[{"role": "user", "content": "è®²ä¸ªæ•…äº‹"}]
):
    print(chunk, end="", flush=True)
```

## ğŸ“‹ æ¨¡å—ä¾èµ–

```
ame/
â”œâ”€â”€ data_processor/      # ç‹¬ç«‹ï¼Œæ— ä¾èµ–
â”œâ”€â”€ vector_store/        # ç‹¬ç«‹ï¼Œå¯é€‰ä¾èµ– chromadb
â”œâ”€â”€ llm_caller/          # ç‹¬ç«‹ï¼Œä¾èµ– openai
â””â”€â”€ rag_generator/       # ä¾èµ– vector_store + llm_caller
```

## ğŸš€ æŠ€æœ¯ç‰¹æ€§

- **æ¨¡å—åŒ–è®¾è®¡**: æ¯ä¸ªæ¨¡å—ç‹¬ç«‹è¿è¡Œï¼Œæ¥å£æ¸…æ™°
- **å·¥å‚æ¨¡å¼**: æ”¯æŒåŠ¨æ€åˆ‡æ¢å®ç°ï¼ˆå¦‚å‘é‡å­˜å‚¨å¼•æ“ï¼‰
- **æ€§èƒ½ä¼˜åŒ–**: ç¼“å­˜æœºåˆ¶ã€å¹¶å‘å¤„ç†ã€é‡è¯•ç­–ç•¥
- **ç±»å‹å®‰å…¨**: å®Œæ•´çš„ç±»å‹æ³¨è§£
- **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°çš„å®ç°

## ğŸ“ ç‰ˆæœ¬ä¿¡æ¯

å½“å‰ç‰ˆæœ¬: **v1.0.0**

**v1.0.0 æ›´æ–°**ï¼š
- âœ¨ åŒæ­¥ä¸»é¡¹ç›®æ¶æ„ä¼˜åŒ–
- âœ¨ æ¨¡å—ç¨³å®šæ€§å¢å¼º
- âœ¨ å®Œå–„æ–‡æ¡£å’Œç¤ºä¾‹

**v0.3.0 æ›´æ–°**ï¼š
- âœ¨ æ–°å¢å¤æ‚æ£€ç´¢æ¨¡å— (retrieval/)
- âœ¨ æ”¯æŒæ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆå‘é‡+å…³é”®è¯+æ—¶é—´ï¼‰
- âœ¨ æ”¯æŒå¤šç§é‡æ’åºç­–ç•¥
- âœ¨ æ‰€æœ‰æ¨¡å—å‡æä¾›æŠ½è±¡åŸºç±»ï¼Œæ”¯æŒè‡ªå®šä¹‰æ‰©å±•

## ğŸ”— ç›¸å…³é“¾æ¥

- [Another Me é¡¹ç›®](../README.md)
- [å®Œæ•´æ–‡æ¡£](../DOCUMENTATION.md)
- [GitHub ä»“åº“](https://github.com/yourusername/another-me)
